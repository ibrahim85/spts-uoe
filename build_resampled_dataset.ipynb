{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Modelling Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = pickle.load(open('data/parsed/weather_dataset_utc.p', 'rb'))\n",
    "\n",
    "weather.set_index(['Timestamp'], inplace=True)\n",
    "weather = weather.resample('5min').mean().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Redistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distributed = pickle.load(open('data/parsed/distributed_dataset_final.p', 'rb')).sort_values(by=['Id', 'Timestamp']).set_index(['Id', 'Timestamp'])\n",
    "distributed = distributed.query('NbBikes != 0').drop(['ShortName', 'Name'], axis=1)\n",
    "distributed.columns = ['DistNbBikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collected = pickle.load(open('data/parsed/collected_dataset_final.p', 'rb')).sort_values(by=['Id', 'Timestamp']).set_index(['Id', 'Timestamp'])\n",
    "collected = collected.query('NbBikes != 0').drop(['ShortName', 'Name'], axis=1)\n",
    "collected.columns = ['CollNbBikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distributed.loc['BikePoints_374'].to_csv('dist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings = pickle.load(open('data/parsed/readings_dataset_utc.p', 'rb'))\n",
    "\n",
    "readings.sort_values(by=['Id', 'Timestamp'], inplace=True)\n",
    "readings.set_index(['Id', 'Timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for station_id in readings.index.get_level_values('Id').unique():\n",
    "    # resample with a freq of 5 mins\n",
    "    resampled_readings = readings.loc[station_id].resample('5min').mean().ffill()\n",
    "    \n",
    "    # merge  weather\n",
    "    merged = resampled_readings.merge(weather, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # merge distributed\n",
    "    if station_id in distributed.index:\n",
    "        dist_resampled = distributed.loc[station_id].resample('5min').sum()\n",
    "        merged = merged.merge(dist_resampled, how='left', left_index=True, right_index=True)\n",
    "        \n",
    "    # merge collected\n",
    "    if station_id in collected.index:\n",
    "        coll_resampled = collected.loc[station_id].resample('5min').sum()\n",
    "        merged = merged.merge(coll_resampled, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # set the id\n",
    "    merged['Id'] = station_id\n",
    "\n",
    "    dfs.append(merged.reset_index())\n",
    "\n",
    "readings_weather = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Column Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifying data types took 0.429966926575\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readings_weather.NbBikes = readings_weather.NbBikes.astype('int16')\n",
    "readings_weather.NbDocks = readings_weather.NbDocks.astype('int16')\n",
    "readings_weather.NbEmptyDocks = readings_weather.NbEmptyDocks.astype('int16')\n",
    "readings_weather.NbUnusableDocks = readings_weather.NbUnusableDocks.astype('int16')\n",
    "readings_weather.DewPt = readings_weather.DewPt.astype('float16')\n",
    "readings_weather.Humidity = readings_weather.Humidity.astype('float16')\n",
    "readings_weather.Pressure = readings_weather.Pressure.astype('float16')\n",
    "readings_weather.Temp = readings_weather.Temp.astype('float16')\n",
    "readings_weather.Visibility = readings_weather.Visibility.astype('float16')\n",
    "readings_weather.WindDirD = readings_weather.WindDirD.astype('float16')\n",
    "readings_weather.WindSpeed = readings_weather.WindSpeed.astype('float16')\n",
    "\n",
    "readings_weather.Fog = readings_weather.Fog.astype('int8')\n",
    "readings_weather.Rain = readings_weather.Rain.astype('int8')\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Modifying data types took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing timestamp took 361.224951982\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readings_weather['Holiday'] = readings_weather.Timestamp.apply(lambda x: x.month == 5 and x.day == 30).astype('int8')\n",
    "readings_weather['Weekday'] = readings_weather.Timestamp.apply(lambda x: x.dayofweek < 5).astype('int8')\n",
    "readings_weather['Weekend'] = readings_weather.Timestamp.apply(lambda x: x.dayofweek > 4).astype('int8')\n",
    "readings_weather['TimeOfYear'] = readings_weather.Timestamp.apply(lambda x: (x - datetime(2016,1,1)).total_seconds())\n",
    "readings_weather['TimeOfDay'] = readings_weather.Timestamp.apply(lambda x: (x - x.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds())\n",
    "readings_weather['WeekOfYear'] = readings_weather.Timestamp.apply(lambda x: x.isocalendar()[1])\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Changing timestamp took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-arrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_weather.sort_values(by=['Id', 'Timestamp'], inplace=True)\n",
    "readings_weather.set_index(['Id', 'Timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_cols(df, cols, periods, mode):\n",
    "    for period in periods:    \n",
    "        target_cols = []\n",
    "        for col in cols:\n",
    "            label = '%s%s%d' % (col, mode, period)\n",
    "            target_cols.append(label)\n",
    "            df[label] = df[col]\n",
    "    \n",
    "        station_ids = df.index.get_level_values('Id').unique()   \n",
    "        for station_id in station_ids:   \n",
    "            shifted = df.loc[station_id, target_cols].shift(periods=period)\n",
    "            df.loc[station_id, target_cols] = shifted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding previous weather columns took 155.414750099\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "shift_cols(readings_weather, ['Temp', 'Humidity', 'Rain', 'Fog'], [1], 'TMinus')\n",
    "shift_cols(readings_weather, ['Temp', 'Humidity'], [12], 'TMinus')\n",
    "end_time = time.time()\n",
    "\n",
    "print 'Adding previous weather columns took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoregressive columns took 380.563352108\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "shift_cols(readings_weather, ['NbBikes'], [1, 2, 12, 18, 24], 'TMinus')\n",
    "end_time = time.time()\n",
    "\n",
    "print 'Adding autoregressive columns took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove\n",
    "\n",
    "Delete stations which experienced periods of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invalid_ids = ['BikePoints_109', 'BikePoints_112', 'BikePoints_120', 'BikePoints_129', 'BikePoints_133', \n",
    "               'BikePoints_153', 'BikePoints_184', 'BikePoints_192', 'BikePoints_218', 'BikePoints_226', \n",
    "               'BikePoints_237', 'BikePoints_260', 'BikePoints_277', 'BikePoints_3', 'BikePoints_31', \n",
    "               'BikePoints_311', 'BikePoints_317', 'BikePoints_323', 'BikePoints_368', 'BikePoints_383', \n",
    "               'BikePoints_386', 'BikePoints_404', 'BikePoints_460', 'BikePoints_476', 'BikePoints_478', \n",
    "               'BikePoints_494', 'BikePoints_497', 'BikePoints_543', 'BikePoints_556', 'BikePoints_583', \n",
    "               'BikePoints_643', 'BikePoints_646', 'BikePoints_672', 'BikePoints_742', 'BikePoints_787', \n",
    "               'BikePoints_790', 'BikePoints_791', 'BikePoints_793', 'BikePoints_796', 'BikePoints_798', \n",
    "               'BikePoints_799', 'BikePoints_802', 'BikePoints_803', 'BikePoints_805', 'BikePoints_807', \n",
    "               'BikePoints_809', 'BikePoints_810', 'BikePoints_811', 'BikePoints_814', 'BikePoints_817', \n",
    "               'BikePoints_818', 'BikePoints_86', 'BikePoints_9']\n",
    "\n",
    "readings_weather.drop(invalid_ids, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 8865854 entries, (BikePoints_1, 2016-05-15 13:35:00+00:00) to (BikePoints_99, 2016-06-26 23:25:00+00:00)\n",
      "Columns: 36 entries, CollNbBikes to NbBikesTMinus24\n",
      "dtypes: float16(11), float64(15), int16(4), int64(1), int8(5)\n",
      "memory usage: 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "readings_weather.info(memory_usage='deep', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(readings_weather, open(\"data/parsed/readings_model.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
