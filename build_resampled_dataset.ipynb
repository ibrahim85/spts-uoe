{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Modelling Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings = pickle.load(open('data/parsed/readings_dataset_utc.p', 'rb'))\n",
    "stations = pickle.load(open('data/parsed/stations_dataset_final.p', 'rb')).set_index('Id')\n",
    "\n",
    "readings.sort_values(by=['Id', 'Timestamp'], inplace=True)\n",
    "readings.set_index(['Id', 'Timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stations [u'BikePoints_237', u'BikePoints_497', u'BikePoints_796', u'BikePoints_805', u'BikePoints_807', u'BikePoints_809', u'BikePoints_811', u'BikePoints_817']\n"
     ]
    }
   ],
   "source": [
    "# remove stations that have less than 21 days of data\n",
    "stations_to_remove = []\n",
    "for station_id in readings.index.get_level_values('Id').unique():\n",
    "    nb_days = len(np.unique(readings.loc[station_id].index.strftime('%Y-%m-%d')))\n",
    "    if nb_days < 21:\n",
    "        stations_to_remove.append(station_id)\n",
    "    \n",
    "readings.drop(stations_to_remove, inplace=True)\n",
    "stations.drop(stations_to_remove, inplace=True)\n",
    "print 'Removed stations %s' % stations_to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closest Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from geopy.distance import vincenty\n",
    "\n",
    "# get the locations of all the stations in tuples\n",
    "stations_locations = pd.Series(zip(stations.Latitude, stations.Longitude), index=stations.index)\n",
    "\n",
    "# create a DF to hold the nearest stations\n",
    "surrounding_stations = pd.DataFrame(columns=['Near%d' % (i+1) for i in range(0,20)], index=stations.index)\n",
    "for station_id, station_loc in stations_locations.iteritems():\n",
    "    ranking = stations_locations.apply(lambda x: vincenty(station_loc, x).kilometers).sort_values()\n",
    "    surrounding_stations.loc[station_id] = ranking.index.values[1:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = pickle.load(open('data/parsed/weather_dataset_utc.p', 'rb'))\n",
    "\n",
    "weather.set_index(['Timestamp'], inplace=True)\n",
    "weather = weather.resample('5min').mean().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Redistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distributed = pickle.load(open('data/parsed/distributed_dataset_final.p', 'rb'))\n",
    "distributed = distributed.query('NbBikes != 0').drop(['Name'], axis=1)\n",
    "distributed.columns = ['DistNbBikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collected = pickle.load(open('data/parsed/collected_dataset_final.p', 'rb'))\n",
    "collected = collected.query('NbBikes != 0').drop(['Name'], axis=1)\n",
    "collected.columns = ['CollNbBikes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for station_id in readings.index.get_level_values('Id').unique():\n",
    "    # resample with a freq of 5 mins\n",
    "    resampled_readings = readings.loc[station_id].resample('5min').mean().ffill()\n",
    "    \n",
    "    # merge weather\n",
    "    merged = resampled_readings.merge(weather, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # merge distributed\n",
    "    if station_id in distributed.index:\n",
    "        dist_resampled = distributed.loc[station_id].resample('5min').sum()\n",
    "        merged = merged.merge(dist_resampled, how='left', left_index=True, right_index=True)\n",
    "        \n",
    "    # merge collected\n",
    "    if station_id in collected.index:\n",
    "        coll_resampled = collected.loc[station_id].resample('5min').sum()\n",
    "        merged = merged.merge(coll_resampled, how='left', left_index=True, right_index=True)\n",
    "        \n",
    "    # merge surrounding\n",
    "    surrounding_stations_nbbikes = []\n",
    "    for surrounding_station_id in surrounding_stations.loc[station_id].values:\n",
    "        surrounding_stations_nbbikes.append(readings.loc[surrounding_station_id].NbBikes.resample('5min').mean().ffill())\n",
    "    surrounding_stations_nbbikes = pd.concat(surrounding_stations_nbbikes, axis=1)\n",
    "    surrounding_stations_nbbikes.columns = surrounding_stations.columns\n",
    "    merged = merged.merge(surrounding_stations_nbbikes, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    # set the id\n",
    "    merged['Id'] = station_id\n",
    "\n",
    "    dfs.append(merged.reset_index())\n",
    "\n",
    "readings_weather = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Column Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifying data types took 5.24216604233\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readings_weather.NbBikes = readings_weather.NbBikes.astype('int16')\n",
    "readings_weather.NbDocks = readings_weather.NbDocks.astype('int16')\n",
    "readings_weather.NbEmptyDocks = readings_weather.NbEmptyDocks.astype('int16')\n",
    "readings_weather.NbUnusableDocks = readings_weather.NbUnusableDocks.astype('int16')\n",
    "readings_weather.CollNbBikes = readings_weather.CollNbBikes.astype('float16')\n",
    "readings_weather.DistNbBikes = readings_weather.DistNbBikes.astype('float16')\n",
    "\n",
    "readings_weather.DistNbBikes = readings_weather.DistNbBikes.fillna(0).astype('int16')\n",
    "readings_weather.CollNbBikes = readings_weather.CollNbBikes.fillna(0).astype('int16')\n",
    "\n",
    "# have to use float due to pandas gotcha with NaN in integer \n",
    "for col in surrounding_stations.columns:\n",
    "    readings_weather[col] = readings_weather[col].astype('float16')\n",
    "    \n",
    "readings_weather.DewPt = readings_weather.DewPt.astype('float16')\n",
    "readings_weather.Humidity = readings_weather.Humidity.astype('float16')\n",
    "readings_weather.Pressure = readings_weather.Pressure.astype('float16')\n",
    "readings_weather.Temp = readings_weather.Temp.astype('float16')\n",
    "readings_weather.Visibility = readings_weather.Visibility.astype('float16')\n",
    "readings_weather.WindDirD = readings_weather.WindDirD.astype('float16')\n",
    "readings_weather.WindSpeed = readings_weather.WindSpeed.astype('float16')\n",
    "\n",
    "readings_weather.Fog = readings_weather.Fog.astype('int8')\n",
    "readings_weather.Rain = readings_weather.Rain.astype('int8')\n",
    "readings_weather.Hail = readings_weather.Hail.astype('int8')\n",
    "readings_weather.Snow = readings_weather.Snow.astype('int8')\n",
    "readings_weather.Thunder = readings_weather.Thunder.astype('int8')\n",
    "readings_weather.Tornado = readings_weather.Tornado.astype('int8')\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Modifying data types took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing timestamp took 398.124156952\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readings_weather['Holiday'] = readings_weather.Timestamp.apply(lambda x: x.month == 5 and x.day == 30).astype('int8')\n",
    "readings_weather['Weekday'] = readings_weather.Timestamp.apply(lambda x: x.dayofweek < 5).astype('int8')\n",
    "readings_weather['Weekend'] = readings_weather.Timestamp.apply(lambda x: x.dayofweek > 4).astype('int8')\n",
    "readings_weather['TimeOfYear'] = readings_weather.Timestamp.apply(lambda x: (x - datetime(2016,1,1)).total_seconds())\n",
    "readings_weather['TimeOfDay'] = readings_weather.Timestamp.apply(lambda x: (x - x.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds())\n",
    "readings_weather['WeekOfYear'] = readings_weather.Timestamp.apply(lambda x: x.isocalendar()[1])\n",
    "readings_weather['DayOfWeek'] = readings_weather.Timestamp.apply(lambda x: x.weekday())\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Changing timestamp took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-arrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_weather.sort_values(by=['Id', 'Timestamp'], inplace=True)\n",
    "readings_weather.set_index(['Id', 'Timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(readings_weather, open(\"data/parsed/readings_resampled.p\", \"wb\"))\n",
    "print 'Saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readings_weather = pickle.load(open('data/parsed/readings_resampled.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_cols(df, cols, periods, mode):\n",
    "    for period in periods:    \n",
    "        target_cols = []\n",
    "        for col in cols:\n",
    "            label = '%s%s%d' % (col, mode, period)\n",
    "            target_cols.append(label)\n",
    "            df[label] = df[col]\n",
    "    \n",
    "        station_ids = df.index.get_level_values('Id').unique()   \n",
    "        for station_id in station_ids:   \n",
    "            shifted = df.loc[station_id, target_cols].shift(periods=period)\n",
    "            df.loc[station_id, target_cols] = shifted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding previous weather columns took 234.748711109\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "shift_cols(readings_weather, ['Temp', 'Humidity', 'Rain', 'Fog'], [2, 12], 'TMinus')\n",
    "\n",
    "readings_weather.TempTMinus2 = readings_weather.TempTMinus2.astype('float16')\n",
    "readings_weather.TempTMinus12 = readings_weather.TempTMinus12.astype('float16')\n",
    "readings_weather.HumidityTMinus2 = readings_weather.HumidityTMinus2.astype('float16')\n",
    "readings_weather.HumidityTMinus12 = readings_weather.HumidityTMinus12.astype('float16')\n",
    "readings_weather.RainTMinus2 = readings_weather.RainTMinus2.astype('float16')\n",
    "readings_weather.RainTMinus12 = readings_weather.RainTMinus12.astype('float16')\n",
    "readings_weather.FogTMinus2 = readings_weather.FogTMinus2.astype('float16')\n",
    "readings_weather.FogTMinus12 = readings_weather.FogTMinus12.astype('float16')\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Adding previous weather columns took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoregressive columns took 321.069705009\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "shift_cols(readings_weather, ['NbBikes'], [1, 2, 3, 12, 18], 'TMinus')\n",
    "\n",
    "readings_weather.NbBikesTMinus1 = readings_weather.NbBikesTMinus1.astype('float16')\n",
    "readings_weather.NbBikesTMinus2 = readings_weather.NbBikesTMinus2.astype('float16')\n",
    "readings_weather.NbBikesTMinus3 = readings_weather.NbBikesTMinus3.astype('float16')\n",
    "readings_weather.NbBikesTMinus12 = readings_weather.NbBikesTMinus12.astype('float16')\n",
    "readings_weather.NbBikesTMinus18 = readings_weather.NbBikesTMinus18.astype('float16')\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Adding autoregressive columns took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoregressive columns took -0.0148899555206\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "shift_cols(readings_weather, ['Near1'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near2'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near3'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near4'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near5'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near6'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near7'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near8'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near9'], [2, 3, 12, 18], 'TMinus')\n",
    "shift_cols(readings_weather, ['Near10'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near11'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near12'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near13'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near14'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near15'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near16'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near17'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near18'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near19'], [2, 3, 12, 18], 'TMinus')\n",
    "#shift_cols(readings_weather, ['Near20'], [2, 3, 12, 18], 'TMinus')\n",
    "end_time = time.time()\n",
    "\n",
    "print 'Adding autoregressive columns took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(readings_weather, open(\"data/parsed/readings_autoregressive.p\", \"wb\"))\n",
    "print 'Saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Historic Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_weather = pickle.load(open('data/parsed/readings_autoregressive.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jfconavarrete/Documents/Work/Config/anaconda2/lib/python2.7/site-packages/pandas/core/generic.py:2698: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding historic average column took 92.1469268799\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readings_weather['HistAvg'] = np.NaN\n",
    "\n",
    "for station_id in readings_weather.index.get_level_values('Id').unique():\n",
    "    window = readings_weather.loc[station_id]\n",
    "    \n",
    "    station_hist_avgs = window.groupby(['Holiday', 'Weekday', 'TimeOfDay'])[['NbBikes']].mean()    \n",
    "    \n",
    "    station_ts = window.reset_index().set_index(['Holiday', 'Weekday', 'TimeOfDay'])[['Timestamp']]\n",
    "    hist_avg_ts = station_ts.merge(station_hist_avgs, how='inner', left_index=True, right_index=True)\n",
    "    hist_avg_ts = hist_avg_ts.reset_index(drop=True).set_index('Timestamp').sort_index().round().NbBikes\n",
    "\n",
    "    readings_weather.loc[station_id].HistAvg = hist_avg_ts\n",
    "    \n",
    "readings_weather.HistAvg = readings_weather.HistAvg.astype('float16')\n",
    "\n",
    "end_time = time.time()\n",
    "print 'Adding historic average column took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9.352339e+06\n",
       "mean     4.326587e+00\n",
       "std      3.752515e+00\n",
       "min      0.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      3.000000e+00\n",
       "75%      6.000000e+00\n",
       "max      4.500000e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "readings_weather.Diff = readings_weather.NbBikes - readings_weather.HistAvg\n",
    "readings_weather.Diff = readings_weather.Diff.apply(math.fabs)\n",
    "readings_weather.Diff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Cumulative Redistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_cumulative(series, n):\n",
    "    cumulative_sum = series\n",
    "    for i in range(1, n + 1):\n",
    "        cumulative_sum = series.shift(i).fillna(0) + cumulative_sum\n",
    "    return cumulative_sum.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding cumulative redistribution columns took 207.053358793\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readings_weather['CollNbBikesCum2'] = np.NaN\n",
    "readings_weather['CollNbBikesCum6'] = np.NaN\n",
    "readings_weather['DistNbBikesCum2'] = np.NaN\n",
    "readings_weather['DistNbBikesCum6'] = np.NaN\n",
    "\n",
    "for station_id in readings_weather.index.get_level_values('Id').unique():\n",
    "    window = readings_weather.loc[station_id]\n",
    "\n",
    "    collected = window.CollNbBikes\n",
    "    readings_weather.loc[station_id].CollNbBikesCum2 = sum_cumulative(collected, 2)\n",
    "    readings_weather.loc[station_id].CollNbBikesCum6 = sum_cumulative(collected, 6)\n",
    "\n",
    "    distributed = window.DistNbBikes\n",
    "    readings_weather.loc[station_id].DistNbBikesCum2 = sum_cumulative(distributed, 2)\n",
    "    readings_weather.loc[station_id].DistNbBikesCum6 = sum_cumulative(distributed, 6)\n",
    "    \n",
    "readings_weather.CollNbBikesCum2 = readings_weather.CollNbBikesCum2.astype('int16')\n",
    "readings_weather.CollNbBikesCum6 = readings_weather.CollNbBikesCum6.astype('int16')\n",
    "readings_weather.DistNbBikesCum2 = readings_weather.DistNbBikesCum2.astype('int16')\n",
    "readings_weather.DistNbBikesCum6 = readings_weather.DistNbBikesCum6.astype('int16')\n",
    "    \n",
    "end_time = time.time()\n",
    "print 'Adding cumulative redistribution columns took %s' % (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(readings_weather, open(\"data/parsed/readings_full.p\", \"wb\"))\n",
    "print 'Saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "\n",
    "Delete stations which experienced periods of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readings_weather = pickle.load(open('data/parsed/readings_full.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_days = pickle.load(open('data/parsed/missing_days.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 27648 invalid readings\n"
     ]
    }
   ],
   "source": [
    "invalid_readings = []\n",
    "for idx, row in missing_days.iterrows():\n",
    "    if idx not in readings_weather.index:\n",
    "        continue\n",
    "    \n",
    "    for missing_day in row.MissingDays:\n",
    "        labels = readings_weather.loc[idx].loc[missing_day:missing_day.replace(hour=23, minute=59, second=59, microsecond=999999)].index\n",
    "        for label in labels:\n",
    "            invalid_readings.append((idx, label))\n",
    "\n",
    "readings_weather.drop(invalid_readings, inplace=True)\n",
    "print 'Removed %d invalid readings' % len(invalid_readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9324691 entries, (BikePoints_1, 2016-05-15 13:35:00+00:00) to (BikePoints_99, 2016-06-26 23:25:00+00:00)\n",
      "Data columns (total 104 columns):\n",
      "CollNbBikes         int16\n",
      "DewPt               float16\n",
      "DistNbBikes         int16\n",
      "Fog                 int8\n",
      "Hail                int8\n",
      "Humidity            float16\n",
      "NbBikes             int16\n",
      "NbDocks             int16\n",
      "NbEmptyDocks        int16\n",
      "NbUnusableDocks     int16\n",
      "Near1               float16\n",
      "Near10              float16\n",
      "Near11              float16\n",
      "Near12              float16\n",
      "Near13              float16\n",
      "Near14              float16\n",
      "Near15              float16\n",
      "Near16              float16\n",
      "Near17              float16\n",
      "Near18              float16\n",
      "Near19              float16\n",
      "Near2               float16\n",
      "Near20              float16\n",
      "Near3               float16\n",
      "Near4               float16\n",
      "Near5               float16\n",
      "Near6               float16\n",
      "Near7               float16\n",
      "Near8               float16\n",
      "Near9               float16\n",
      "Pressure            float16\n",
      "Rain                int8\n",
      "Snow                int8\n",
      "Temp                float16\n",
      "Thunder             int8\n",
      "Tornado             int8\n",
      "Visibility          float16\n",
      "WindDirD            float16\n",
      "WindSpeed           float16\n",
      "Holiday             int8\n",
      "Weekday             int8\n",
      "Weekend             int8\n",
      "TimeOfYear          float64\n",
      "TimeOfDay           float64\n",
      "WeekOfYear          int64\n",
      "DayOfWeek           int64\n",
      "TempTMinus2         float16\n",
      "HumidityTMinus2     float16\n",
      "RainTMinus2         float16\n",
      "FogTMinus2          float16\n",
      "TempTMinus12        float16\n",
      "HumidityTMinus12    float16\n",
      "RainTMinus12        float16\n",
      "FogTMinus12         float16\n",
      "NbBikesTMinus1      float16\n",
      "NbBikesTMinus2      float16\n",
      "NbBikesTMinus3      float16\n",
      "NbBikesTMinus12     float16\n",
      "NbBikesTMinus18     float16\n",
      "Near1TMinus2        float16\n",
      "Near1TMinus3        float16\n",
      "Near1TMinus12       float16\n",
      "Near1TMinus18       float16\n",
      "Near2TMinus2        float16\n",
      "Near2TMinus3        float16\n",
      "Near2TMinus12       float16\n",
      "Near2TMinus18       float16\n",
      "Near3TMinus2        float16\n",
      "Near3TMinus3        float16\n",
      "Near3TMinus12       float16\n",
      "Near3TMinus18       float16\n",
      "Near4TMinus2        float16\n",
      "Near4TMinus3        float16\n",
      "Near4TMinus12       float16\n",
      "Near4TMinus18       float16\n",
      "Near5TMinus2        float16\n",
      "Near5TMinus3        float16\n",
      "Near5TMinus12       float16\n",
      "Near5TMinus18       float16\n",
      "Near6TMinus2        float16\n",
      "Near6TMinus3        float16\n",
      "Near6TMinus12       float16\n",
      "Near6TMinus18       float16\n",
      "Near7TMinus2        float16\n",
      "Near7TMinus3        float16\n",
      "Near7TMinus12       float16\n",
      "Near7TMinus18       float16\n",
      "Near8TMinus2        float16\n",
      "Near8TMinus3        float16\n",
      "Near8TMinus12       float16\n",
      "Near8TMinus18       float16\n",
      "Near9TMinus2        float16\n",
      "Near9TMinus3        float16\n",
      "Near9TMinus12       float16\n",
      "Near9TMinus18       float16\n",
      "Near10TMinus2       float16\n",
      "Near10TMinus3       float16\n",
      "Near10TMinus12      float16\n",
      "Near10TMinus18      float16\n",
      "HistAvg             float16\n",
      "CollNbBikesCum2     int16\n",
      "CollNbBikesCum6     int16\n",
      "DistNbBikesCum2     int16\n",
      "DistNbBikesCum6     int16\n",
      "dtypes: float16(81), float64(2), int16(10), int64(2), int8(9)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "readings_weather.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(readings_weather, open(\"data/parsed/readings_clean.p\", \"wb\"))\n",
    "print 'Saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
