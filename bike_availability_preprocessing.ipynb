{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bike Availability Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "The raw data contains the following data per station per reading:\n",
    "\n",
    "* Id - String - API Resource Id\n",
    "* Name - String - The common name of the station\n",
    "* PlaceType - String ?\n",
    "* TerminalName - String - ?\n",
    "* NbBikes - Integer - The number of available bikes\n",
    "* NbDocks - Integer - The total number of docking spaces\n",
    "* NbEmptyDocks - Integer - The number of available empty docking spaces\n",
    "* Timestamp - DateTime - The moment this reading was captured\n",
    "* InstallDate - DateTime - Date when the station was installed\n",
    "* RemovalDate - DateTime - Date when the station was removed\n",
    "* Installed - Boolean - If the station is installed or not\n",
    "* Locked - Boolean - ?\n",
    "* Temporary - Boolean - If the station is temporary or not (TfL adds temporary stations to cope with demand.)\n",
    "* Latitude - Float - Latitude Coordinate\n",
    "* Longitude - Float - Longitude Coordinate\n",
    "\n",
    "The following variables will be derived from the raw data.\n",
    "\n",
    "* NbUnusableDocks - Integer - The number of non-working docking spaces. Computed with NbUnusableDocks = NbDocks - (NbBikes + NbEmptyDocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from IPython.display import Image\n",
    "from datetime import date\n",
    "\n",
    "from src.data.parse_dataset import parse_dir, parse_json_files, get_file_list\n",
    "from src.data.string_format import format_name, to_short_name\n",
    "from src.data.visualization import lon_min_longitude, lon_min_latitude, lon_max_longitude, lon_max_latitude, lon_center_latitude, lon_center_longitude, create_london_map\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Raw Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_cycles(json_obj):\n",
    "    \"\"\"Parses TfL's BikePoint JSON response\"\"\"\n",
    "\n",
    "    return [parse_station(element) for element in json_obj]\n",
    "\n",
    "def parse_station(element):\n",
    "    \"\"\"Parses a JSON bicycle station object to a dictionary\"\"\"\n",
    "\n",
    "    obj = {\n",
    "        'Id': element['id'],\n",
    "        'Name': element['commonName'],\n",
    "        'Latitude': element['lat'],\n",
    "        'Longitude': element['lon'],\n",
    "        'PlaceType': element['placeType'],\n",
    "    }\n",
    "\n",
    "    for p in element['additionalProperties']:\n",
    "        obj[p['key']] = p['value']\n",
    "\n",
    "        if 'timestamp' not in obj:\n",
    "            obj['Timestamp'] = p['modified']\n",
    "        elif obj['Timestamp'] != p['modified']:\n",
    "            raise ValueError('The properties\\' timestamps for station %s do not match: %s != %s' % (\n",
    "            obj['id'], obj['Timestamp'], p['modified']))\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bike_file_date_fn(file_name):\n",
    "    \"\"\"Gets the file's date\"\"\"\n",
    "\n",
    "    return datetime.strptime(os.path.basename(file_name), 'BIKE-%Y-%m-%d:%H:%M:%S.json')\n",
    "\n",
    "def create_between_dates_filter(file_date_fn, date_start, date_end):\n",
    "    def filter_fn(file_name):\n",
    "        file_date = file_date_fn(file_name)\n",
    "        return file_date >= date_start and file_date <= date_end\n",
    "    \n",
    "    return filter_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Data View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Single Day Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_fn = create_between_dates_filter(bike_file_date_fn, \n",
    "                                       datetime(2016, 5, 16, 7, 0, 0),\n",
    "                                       datetime(2016, 5, 16, 23, 59, 59))\n",
    "\n",
    "records = parse_dir('/home/jfconavarrete/Documents/Work/Dissertation/spts-uoe/data/raw/cycles', \n",
    "                    parse_cycles, sort_fn=bike_file_date_fn, filter_fn=filter_fn)\n",
    "\n",
    "# records is a list of lists of dicts\n",
    "df = pd.DataFrame(list(itertools.chain.from_iterable(records))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  All Station View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>InstallDate</th>\n",
       "      <th>Installed</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Locked</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Name</th>\n",
       "      <th>NbBikes</th>\n",
       "      <th>NbDocks</th>\n",
       "      <th>NbEmptyDocks</th>\n",
       "      <th>PlaceType</th>\n",
       "      <th>RemovalDate</th>\n",
       "      <th>Temporary</th>\n",
       "      <th>TerminalName</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BikePoints_1</td>\n",
       "      <td>1278947280000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001023</td>\n",
       "      <td>2016-05-16T06:26:24.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BikePoints_2</td>\n",
       "      <td>1278585780000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.499606</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.197574</td>\n",
       "      <td>Phillimore Gardens, Kensington</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001018</td>\n",
       "      <td>2016-05-16T06:26:24.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BikePoints_3</td>\n",
       "      <td>1278240360000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.521283</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>Christopher Street, Liverpool Street</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001012</td>\n",
       "      <td>2016-05-16T06:51:27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BikePoints_4</td>\n",
       "      <td>1278241080000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.530059</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.120973</td>\n",
       "      <td>St. Chad's Street, King's Cross</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001013</td>\n",
       "      <td>2016-05-16T06:51:27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BikePoints_5</td>\n",
       "      <td>1278241440000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.493130</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.156876</td>\n",
       "      <td>Sedding Street, Sloane Square</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>003420</td>\n",
       "      <td>2016-05-16T06:46:27.237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    InstallDate Installed   Latitude Locked  Longitude  \\\n",
       "0  BikePoints_1  1278947280000      true  51.529163  false  -0.109970   \n",
       "1  BikePoints_2  1278585780000      true  51.499606  false  -0.197574   \n",
       "2  BikePoints_3  1278240360000      true  51.521283  false  -0.084605   \n",
       "3  BikePoints_4  1278241080000      true  51.530059  false  -0.120973   \n",
       "4  BikePoints_5  1278241440000      true  51.493130  false  -0.156876   \n",
       "\n",
       "                                   Name NbBikes NbDocks NbEmptyDocks  \\\n",
       "0            River Street , Clerkenwell      11      19            7   \n",
       "1        Phillimore Gardens, Kensington      12      37           25   \n",
       "2  Christopher Street, Liverpool Street       6      32           26   \n",
       "3       St. Chad's Street, King's Cross      14      23            9   \n",
       "4         Sedding Street, Sloane Square      27      27            0   \n",
       "\n",
       "   PlaceType RemovalDate Temporary TerminalName                Timestamp  \n",
       "0  BikePoint                 false       001023  2016-05-16T06:26:24.037  \n",
       "1  BikePoint                 false       001018  2016-05-16T06:26:24.037  \n",
       "2  BikePoint                 false       001012    2016-05-16T06:51:27.5  \n",
       "3  BikePoint                 false       001013    2016-05-16T06:51:27.5  \n",
       "4  BikePoint                 false       003420  2016-05-16T06:46:27.237  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Single Station View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>InstallDate</th>\n",
       "      <th>Installed</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Locked</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Name</th>\n",
       "      <th>NbBikes</th>\n",
       "      <th>NbDocks</th>\n",
       "      <th>NbEmptyDocks</th>\n",
       "      <th>PlaceType</th>\n",
       "      <th>RemovalDate</th>\n",
       "      <th>Temporary</th>\n",
       "      <th>TerminalName</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BikePoints_1</td>\n",
       "      <td>1278947280000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.10997</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001023</td>\n",
       "      <td>2016-05-16T06:26:24.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>BikePoints_1</td>\n",
       "      <td>1278947280000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.10997</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001023</td>\n",
       "      <td>2016-05-16T06:26:24.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>BikePoints_1</td>\n",
       "      <td>1278947280000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.10997</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001023</td>\n",
       "      <td>2016-05-16T07:01:29.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>BikePoints_1</td>\n",
       "      <td>1278947280000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.10997</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001023</td>\n",
       "      <td>2016-05-16T07:11:30.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>BikePoints_1</td>\n",
       "      <td>1278947280000</td>\n",
       "      <td>true</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>false</td>\n",
       "      <td>-0.10997</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>BikePoint</td>\n",
       "      <td></td>\n",
       "      <td>false</td>\n",
       "      <td>001023</td>\n",
       "      <td>2016-05-16T07:11:30.433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id    InstallDate Installed   Latitude Locked  Longitude  \\\n",
       "0     BikePoints_1  1278947280000      true  51.529163  false   -0.10997   \n",
       "762   BikePoints_1  1278947280000      true  51.529163  false   -0.10997   \n",
       "1524  BikePoints_1  1278947280000      true  51.529163  false   -0.10997   \n",
       "2286  BikePoints_1  1278947280000      true  51.529163  false   -0.10997   \n",
       "3048  BikePoints_1  1278947280000      true  51.529163  false   -0.10997   \n",
       "\n",
       "                            Name NbBikes NbDocks NbEmptyDocks  PlaceType  \\\n",
       "0     River Street , Clerkenwell      11      19            7  BikePoint   \n",
       "762   River Street , Clerkenwell      11      19            7  BikePoint   \n",
       "1524  River Street , Clerkenwell      10      19            8  BikePoint   \n",
       "2286  River Street , Clerkenwell       8      19           10  BikePoint   \n",
       "3048  River Street , Clerkenwell       8      19           10  BikePoint   \n",
       "\n",
       "     RemovalDate Temporary TerminalName                Timestamp  \n",
       "0                    false       001023  2016-05-16T06:26:24.037  \n",
       "762                  false       001023  2016-05-16T06:26:24.037  \n",
       "1524                 false       001023  2016-05-16T07:01:29.163  \n",
       "2286                 false       001023  2016-05-16T07:11:30.433  \n",
       "3048                 false       001023  2016-05-16T07:11:30.433  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Id'] == 'BikePoints_1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* There are some duplicate rows <- remove duplicates\n",
    "* RemovalDate may contain a lot of nulls <- remove if not helpful\n",
    "* Locked and Installed might be constant <- remove if not helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory constraints we'll parse the data in chunks. In each chunk we'll remove the redundant candidate keys and also duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables\n",
    "\n",
    "We will have two different tables, one for the stations and one for the availability readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(parsed_data):\n",
    "    master_df = pd.DataFrame(list(itertools.chain.from_iterable(parsed_data)))\n",
    "    \n",
    "    readings_df = pd.DataFrame(master_df, columns=['Id', 'Timestamp', 'NbBikes', 'NbDocks', 'NbEmptyDocks'])\n",
    "    stations_df = pd.DataFrame(master_df, columns=['Id', 'Name', 'TerminalName' , 'PlaceType', 'Latitude', \n",
    "                                                   'Longitude', 'Installed', 'Temporary', 'Locked',\n",
    "                                                   'RemovalDate', 'InstallDate'])\n",
    "    \n",
    "    return (readings_df, stations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the files to parse\n",
    "five_weekdays_filter = create_between_dates_filter(bike_file_date_fn, \n",
    "                                                   datetime(2016, 6, 19, 0, 0, 0), \n",
    "                                                   datetime(2016, 6, 27, 23, 59, 59))\n",
    "\n",
    "files = get_file_list('data/raw/cycles', filter_fn=None, sort_fn=bike_file_date_fn)\n",
    "\n",
    "# process the files in chunks\n",
    "files_batches = chunker(files, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start with an empty dataset\n",
    "readings_dataset = pd.DataFrame()\n",
    "stations_dataset = pd.DataFrame()\n",
    "\n",
    "# append each chunk to the datasets while removing duplicates\n",
    "for batch in files_batches:\n",
    "    parsed_data = parse_json_files(batch, parse_cycles)\n",
    "    \n",
    "    # split the data into two station data and readings data\n",
    "    readings_df, stations_df = split_data(parsed_data)\n",
    "    \n",
    "    # append the datasets\n",
    "    readings_dataset = pd.concat([readings_dataset, readings_df])\n",
    "    stations_dataset = pd.concat([stations_dataset, stations_df])\n",
    "    \n",
    "    # remove duplicated rows\n",
    "    readings_dataset.drop_duplicates(inplace=True)\n",
    "    stations_dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the parsed data in pickle files\n",
    "pickle.dump(readings_dataset, open(\"data/parsed/readings_dataset_raw.p\", \"wb\"))\n",
    "pickle.dump(stations_dataset, open(\"data/parsed/stations_dataset_raw.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-edb8348384ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstations_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/parsed/stations_dataset_raw.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreadings_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/parsed/readings_dataset_raw.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "stations_dataset = pickle.load(open('data/parsed/stations_dataset_raw.p', 'rb'))\n",
    "readings_dataset = pickle.load(open('data/parsed/readings_dataset_raw.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technically Correct Data\n",
    "\n",
    "The data is set to be technically correct if it:\n",
    "\n",
    "1. can be directly recognized as belonging to a certain variable\n",
    "2. is stored in a data type that represents the value domain of the real-world variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert columns to their appropriate datatypes\n",
    "stations_dataset['InstallDate'] = pd.to_numeric(stations_dataset['InstallDate'], errors='raise')\n",
    "stations_dataset['RemovalDate'] = pd.to_numeric(stations_dataset['RemovalDate'], errors='raise')\n",
    "\n",
    "stations_dataset['Installed'].replace({'true': True, 'false': False}, inplace=True)\n",
    "stations_dataset['Temporary'].replace({'true': True, 'false': False}, inplace=True)\n",
    "stations_dataset['Locked'].replace({'true': True, 'false': False}, inplace=True)\n",
    "\n",
    "readings_dataset['NbBikes'] = readings_dataset['NbBikes'].astype('uint16')\n",
    "readings_dataset['NbDocks'] = readings_dataset['NbDocks'].astype('uint16')\n",
    "readings_dataset['NbEmptyDocks'] = readings_dataset['NbEmptyDocks'].astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# format station name\n",
    "stations_dataset['Name'] = stations_dataset['Name'].apply(format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert string timestamp to datetime\n",
    "stations_dataset['InstallDate'] = pd.to_datetime(stations_dataset['InstallDate'], unit='ms', errors='raise')\n",
    "stations_dataset['RemovalDate'] = pd.to_datetime(stations_dataset['RemovalDate'], unit='ms', errors='raise')\n",
    "\n",
    "readings_dataset['Timestamp'] =  pd.to_datetime(readings_dataset['Timestamp'], format='%Y-%m-%dT%H:%M:%S.%f', errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the datasets\n",
    "stations_dataset.sort_values(by=['Id'], ascending=True, inplace=True)\n",
    "\n",
    "readings_dataset.sort_values(by=['Timestamp'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stations_dataset['ShortName'] = stations_dataset['Name'].apply(to_short_name)\n",
    "\n",
    "readings_dataset['NbUnusableDocks'] = readings_dataset['NbDocks'] - (readings_dataset['NbBikes'] + readings_dataset['NbEmptyDocks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Station Priority Column\n",
    "Priorities downloaded from https://www.whatdotheyknow.com/request/tfl_boris_bike_statistics?unfold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations_priorities = pd.read_csv('data/raw/priorities/station_priorities.csv', encoding='latin-1')\n",
    "stations_priorities['Site'] = stations_priorities['Site'].apply(format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset = pd.merge(stations_dataset, stations_priorities, how='left', left_on='ShortName', right_on='Site')\n",
    "stations_dataset['Priority'].replace({'One': '1', 'Two': '2', 'Long Term Suspended': np.NaN, 'Long term suspension': np.NaN}, inplace=True)\n",
    "stations_dataset.drop(['Site'], axis=1, inplace=True)\n",
    "stations_dataset.drop(['Borough'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistent Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations Analysis\n",
    "\n",
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stations_dataset.apply(lambda x:x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "* Id, Name and Terminal name seem to be candidate keys\n",
    "* The minimum latitude and the maximum longitude are 0\n",
    "* Some stations have the same latitude or longitude\n",
    "* Id, TerminalName and Name have different unique values\n",
    "* Placetype, Installed, Temporary and Locked appear to be constant\n",
    "* Some stations do not have an install date\n",
    "* Some Stations have a removal date (very sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_duplicate_ids(df):\n",
    "    \"\"\"Find Ids that have more than one value in the given columns\"\"\"\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    value_counts_grouped_by_id = df.groupby('Id').count()    \n",
    "    is_duplicate_id = value_counts_grouped_by_id.applymap(lambda x: x > 1).any(axis=1)\n",
    "    duplicate_ids = value_counts_grouped_by_id[is_duplicate_id == True].index.values\n",
    "    return df[df['Id'].isin(duplicate_ids)]\n",
    "\n",
    "diplicate_ids = find_duplicate_ids(stations_dataset)\n",
    "diplicate_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these records have the same location and Id but different Name or TerminalName, we'll assume the station changed name and remove the first entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the one not in merchant street\n",
    "stations_dataset.drop(417, inplace=True)\n",
    "\n",
    "# remove the one with the shortest name\n",
    "stations_dataset.drop(726, inplace=True)\n",
    "\n",
    "# remove the one that is not in kings cross (as the name of the station implies)\n",
    "stations_dataset.drop(745, inplace=True)\n",
    "\n",
    "# remove the duplicated entries \n",
    "stations_dataset.drop([747, 743, 151, 754, 765, 768],  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure there are no repeated ids \n",
    "assert len(find_duplicate_ids(stations_dataset)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the station locations. All of them should be in Greater London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_locations_outside_box(locations, min_longitude, min_latitude, max_longitude, max_latitude):\n",
    "    latitude_check = ~(locations['Latitude'] >= min_latitude) & (locations['Latitude'] <= max_latitude) \n",
    "    longitude_check = ~(locations['Longitude'] >= min_longitude) & (locations['Longitude'] <= max_longitude) \n",
    "    return locations[(latitude_check | longitude_check)]\n",
    "\n",
    "outlier_locations_df = find_locations_outside_box(stations_dataset, lon_min_longitude, lon_min_latitude, \n",
    "                                                  lon_max_longitude, lon_max_latitude)\n",
    "outlier_locations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This station looks like a test dation, so we'll remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outlier_locations_idx = outlier_locations_df.index.values\n",
    "\n",
    "stations_dataset.drop(outlier_locations_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure there are no stations outside London\n",
    "assert len(find_locations_outside_box(stations_dataset, lon_min_longitude, lon_min_latitude, \n",
    "                                      lon_max_longitude, lon_max_latitude)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate the fact that there are stations with duplicate latitude or longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find stations with duplicate longitude\n",
    "id_counts_groupedby_longitude = stations_dataset.groupby('Longitude')['Id'].count()\n",
    "nonunique_longitudes = id_counts_groupedby_longitude[id_counts_groupedby_longitude != 1].index.values\n",
    "nonunique_longitude_stations = stations_dataset[stations_dataset['Longitude'].isin(nonunique_longitudes)].sort_values(by=['Longitude'])\n",
    "\n",
    "id_counts_groupedby_latitude = stations_dataset.groupby('Latitude')['Id'].count()\n",
    "nonunique_latitudes = id_counts_groupedby_latitude[id_counts_groupedby_latitude != 1].index.values\n",
    "nonunique_latitudes_stations = stations_dataset[stations_dataset['Latitude'].isin(nonunique_latitudes)].sort_values(by=['Latitude'])\n",
    "\n",
    "nonunique_coordinates_stations = pd.concat([nonunique_longitude_stations, nonunique_latitudes_stations])\n",
    "nonunique_coordinates_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_stations_map(stations_df):    \n",
    "    stations_map = create_london_map()\n",
    "\n",
    "    for index, station in stations_df.iterrows():        \n",
    "        folium.Marker([station['Latitude'],station['Longitude']], popup=station['Name']).add_to(stations_map)\n",
    "    \n",
    "    return stations_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_stations_map(nonunique_coordinates_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the stations are different and that having the same Longitude is just a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the stations in a map to see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "london_longitude = -0.127722\n",
    "london_latitude = 51.507981\n",
    "\n",
    "MAX_RECORDS = 100\n",
    "\n",
    "stations_map = create_london_map()\n",
    "\n",
    "for index, station in stations_dataset[0:MAX_RECORDS].iterrows():\n",
    "    folium.Marker([station['Latitude'],station['Longitude']], popup=station['Name']).add_to(stations_map)\n",
    "    \n",
    "stations_map\n",
    "\n",
    "#folium.Map.save(stations_map, 'reports/maps/stations_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings Analysis\n",
    "\n",
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.apply(lambda x:x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamps = readings_dataset['Timestamp']\n",
    "ax = timestamps.groupby([timestamps.dt.year, timestamps.dt.month, timestamps.dt.day]).count().plot(kind=\"bar\")\n",
    "ax.set_xlabel('Days')\n",
    "ax.set_title('Readings per Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "* The number of readings in each day varies widely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard Out of Range Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = date(2016, 5, 16)\n",
    "end_date = date(2016, 6, 27)\n",
    "days = set(pd.date_range(start=start_date, end=end_date, closed='left'))\n",
    "           \n",
    "readings_dataset = readings_dataset[(timestamps > start_date) & (timestamps < end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readings Consistency Through Days\n",
    "Lets get some insight about which stations do not have readings during an entire day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get a subview of the readings dataset\n",
    "id_timestamp_view = readings_dataset.loc[:,['Id','Timestamp']]\n",
    "\n",
    "# remove the time component of the timestamp\n",
    "id_timestamp_view['Timestamp'] = id_timestamp_view['Timestamp'].apply(lambda x: x.replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "\n",
    "# compute the days of readings per stations\n",
    "days_readings = id_timestamp_view.groupby('Id').aggregate(lambda x: set(x))\n",
    "days_readings['MissingDays'] = days_readings['Timestamp'].apply(lambda x: list(days - x))\n",
    "days_readings['MissingDaysCount'] = days_readings['MissingDays'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expand_datetime(df, datetime_col):\n",
    "    df['Weekday'] = df[datetime_col].apply(lambda x: x.weekday())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the stations with missing readings only\n",
    "missing_days_readings = days_readings[days_readings['MissingDaysCount'] != 0]\n",
    "missing_days_readings = missing_days_readings['MissingDays'].apply(lambda x: pd.Series(x)).unstack().dropna()\n",
    "missing_days_readings.index = missing_days_readings.index.droplevel()\n",
    "\n",
    "# sort and format in their own DF\n",
    "missing_days_readings = pd.DataFrame(missing_days_readings, columns=['MissingDay'], index=None).reset_index().sort_values(by=['Id', 'MissingDay'])\n",
    "\n",
    "# expand the missing day date\n",
    "expand_datetime(missing_days_readings, 'MissingDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_days_readings['Id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the missing readings days \n",
    "days = missing_days_readings['MissingDay']\n",
    "missing_days_counts = days.groupby([days.dt.year, days.dt.month, days.dt.day]).count()\n",
    "ax = missing_days_counts.plot(kind=\"bar\")\n",
    "ax.set_xlabel('Days')\n",
    "ax.set_title('Missing Readings Days Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations with no readings in at least one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_days_readings_stations = stations_dataset[stations_dataset['Id'].isin(missing_days_readings['Id'].unique())]\n",
    "draw_stations_map(missing_days_readings_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Stations with no readings in at least one day during the weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekend_readings = missing_days_readings[missing_days_readings['Weekday'] > 4]\n",
    "missing_dayreadings_stn = stations_dataset[stations_dataset['Id'].isin(weekend_readings['Id'].unique())]\n",
    "draw_stations_map(missing_dayreadings_stn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Stations with no readings in at least one day during weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekday_readings = missing_days_readings[missing_days_readings['Weekday'] < 5]\n",
    "missing_dayreadings_stn = stations_dataset[stations_dataset['Id'].isin(weekday_readings['Id'].unique())]\n",
    "draw_stations_map(missing_dayreadings_stn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* There are 29 stations that do not have readings in at least one day\n",
    "* There were more stations without readings during May than in June\n",
    "* Other than that, there is no visible pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard Non Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour = readings_dataset['Timestamp'].apply(lambda x: x.hour)\n",
    "selector = (hour < 7) | (hour > 22)\n",
    "#readings_dataset.drop(readings_dataset[selector].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Datasets\n",
    "### Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readings_dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readings_dataset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(readings_dataset, open(\"data/parsed/readings_dataset_final.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations_dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_dataset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(stations_dataset, open(\"data/parsed/stations_dataset_final.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
